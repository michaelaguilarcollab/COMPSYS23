\section{Discussion}

In the era of massively multicore computing, where it is commonplace to see node architectures with dozens of cores, colocating system or user services with compute tasks is usually seen as a minor imposition on a running application. However, it is clear that using typical scheduling tactics and process layouts can significantly impact the runtime of applications. This has been demonstrated to be especially true at scale. How can we mitigate these impacts while continuing to offer useful services to HPC users?

Any solution compute interference should understand the requirements of the application and user expectation. While this work looks at the problem only from the perspective of compute impact, there is the related concern of I/O impact. Any compute impact mitigation has the potential to impact storage performance, which may be the more important to some users. Therefore, we encourage multiple, possibly conflicting mitigations to be made available to allow maximum flexibility for the end user.

One long-used mitigation strategy is core specialization, where some cores are dedicated to specific system services. This is becoming more pertinent with the advent of efficiency cores in modern CPUs. The main idea is that computations are pinned to some number of cores that are dedicated to their use, while other services are scheduled onto a separate subset of cores, allowing for less interference between them. One downside of this strategy is its static nature. A job submission is typically specified as the number of processes per node, so the split between specialized cores and service cores must be decided in advance. Over-estimating or under-estimating the number of cores required for services will constrain the available performance of the system. However, using so-called ``efficiency cores'' for services may mitigate this somewhat, as these cores are more tuned to handing tasks like I/O, creating a natural place to draw the line between compute and services. This can work well if overheads are expected to consume at least a full core, and compute task performance is important.

In absence of core specialization, CPU and network quotas can provide another means for limiting the impact of storage on compute. Simply indicating to the operating system that storage daemons may only consume some percentage of resources would control the maximum impact of these services. Relatedly, creating a mechanism to throttle network traffic from file system clients at the source will go some distance toward making the storage system self-regulating. Unfortunately, this would do little to mitigate hot spots, where all clients are attempting to access the same small set of servers for storage services. For tightly coupled, balanced jobs, that could cause broad performance degradation.

Another strategy for ensuring minimal application impact is to allow users to control where file system processes are located within a job. In the case of the example benchmark in this paper, simply starting all BeeOND processes on nodes that were engaged in IOR runs would exempt HPL-running nodes from any impacts. A downside for typical HPC architectures with direct-attached storage is the loss of use of SSDs kept within those nodes, causing underutilization and smaller/slower file systems. While it may be possible to use network protocols such as NVMe Over Fabrics on exempt nodes to share the block storage with the nodes running daemons, the host node may still experience some runtime impacts. Less risky but more costly solutions include architectures with disaggregated storage, designed as Just-a-Bunch-Of-Flash chassis. One can also simply expand the size of the job to include extra servers specifically for file system services, if the user anticipates high utilization.
