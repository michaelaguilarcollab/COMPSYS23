\section{Introduction}

Traditional HPC compute clusters are created by combining separate compute servers over network fabric devices to form the cluster.  Each individual compute server is provisioned with its own CPUs, memory devices, accelerator cards, and storage devices to incorporate as many different application runtime requirements as possible.\cite{beowulf} This need to incorporate 'all of the options' results in resource overprovisioning, makes traditional HPC architectures less flexible and more inefficient, and can lead to situations where application jobs more prone to run-time failure.    

For example, design considerations that lead to an under-estimation of compute server memory resources can cause out-of-memory conditions.  In another example, IO server memory oversubscription can result in filesystem failure can occur due to virtual memory page swap thrashing and eventually application failure when the dynamic addition of memory would be able to help mitigate this problem.  

Another issue with the architectural inflexibility of current, siloed, HPC architecture is that it frequently results in overprovisioned or stranded resources.  Stranded resources are those that are either are on a compute server that is unavailable to a workload, or that have been assigned to a workload that isn't making use of them and are unavailable to be used by other workloads. Overprovisioned resources are those that are either underused, or unused and idle for the current workloads but still draw energy and cooling.  Energy wasted in data centers is becoming an increasingly important issue.\cite{eere}

The facility costs of large scale HPC systems including cooling and energy usage is becoming more of an issue. Today, 4 percent of energy produced in the world is used in data centers, up from 2 percent of energy usage was used in data centers, 2 years ago. \cite{dw} \cite{vmware} 

A solution to addressing the overprovisioning and computational efficiency limitations, as well as hardware and operating costs, of integrated, siloed, systems is the use of Composable Disaggregated Infrastructures. (Figure \ref{fig:stranded}) 

\begin{figure}
\centerline{\includegraphics[width=\columnwidth]{Slide3.jpeg}}
\caption{More Efficiency in Composable HPC Use of Resources.} 
\label{fig:stranded}
\end{figure}

With Composable Disaggregated Infrastructures, computational resources are not statically provisioned in servers, but instead are physically disaggregated and connected through high-speed/low-latency network fabrics.  These resources can be dynamically provisioned and reprovisioned to client applications, as needed and are thus not only more efficient to manage by removing unnecessary hardware, but help reduce energy consumption and datacenter cooling costs.  In this type of architecture, shared 'pools' are created that are accessed across high-speed, low latency fabrics. Figure \ref{fig:Pools}. 


\begin{figure}
\centerline{\includegraphics[width=\columnwidth]{Slide4.jpeg}}
\caption{Localized Disaggregated Resource Pools Connected by Fabrics.} 
\label{fig:Pools}
\end{figure}
  
Network disaggregation is already common for storage devices (e.g., NVMe-oF); current trends are pushing this paradigm further, extending it to computational engines, memory elements, accelerators… eventually to all forms of compute resources required by modern HPC applications.  However, disaggregated resource types are increasingly being accessed over an increasing number of fabric types and technologies; and being able to fully manage these resources in a dynamic, heterogenous environment requires managing those fabrics and the hardware resources that may be accessed thereon. The management and optimization of such a diverse set of fabrics and fabric technologies to realize the benefits of Composable Disaggregated Infrastructures is quickly becoming a complex issue to solve for infrastructure managers, especially in heterogenous multi-vendor environments, with multiple vendor-sourced hardware and the ever-expanding collection of proprietary APIs and tools. Currently, there is no common open-source manager interface or model available to configure the resource pools and the fabrics that link them with applications that need them. So every tool & every middleware library provider needs unique calls to specific fabric managements stack for each available fabric You end up with very diverse administration domains w/ administrators having to manage each fabric differently through different tools.

The industry needs interoperatbility through common interfaces to enable Composability Managers to efficiently connect workloads with resources in a dynamic ecosystem while being able to the abstractly control the underlying network interconnect.  This paper describes an OpenFabric Management Framework (OFMF).  The OFMF is an open-source API, tool set, and central repository being designed and developed for centralized management of composable resources over dissimilar fabrics and for manipulation of connected resources using client-friendly abstractions.  The OFMF provides infrastructure that makes it possible to dynamically configure fabric interconects to pair client workloads.

  


