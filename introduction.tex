\section{Introduction}

Traditional HPC compute clusters are created by combining separate compute servers over network fabric devices to form the cluster.  Each individual compute server is statically provisioned with its own CPUs, memory devices, accelerator cards, and storage devices to incorporate as many different application runtime requirements as possible\cite{beowulf}. This need to incorporate 'all of the options' often results in resource overprovisioning, makes traditional HPC architectures less flexible and more inefficient, and can lead to situations where application jobs are more prone to run-time failure. This paper target HPC infrastructure, however it is worth to note that resources overprovisioning and inefficient use of hardware are common issues to any large scale computing facility \cite{borg-google, pond}.

For example, design considerations that lead to an under-estimation of compute server memory resources can cause out-of-memory conditions.  In another example, IO server memory oversubscription can result in filesystem failure can occur due to virtual memory page swap thrashing, and eventually application failure when the dynamic addition of memory would be able to help mitigate this problem.  

Another issue with the architectural inflexibility of current, siloed, HPC architecture is that it frequently results in overprovisioned or stranded resources. Stranded resources are those that are either are on a compute server that, due to a lack of other resources (e.g., CPU), is unavailable to a workload, or that have been assigned to a workload that isn't making use of them and are unavailable to be used by other workloads. Overprovisioned resources are those that are either underused, or unused and idle for the current workloads but still draw energy and cooling. Energy wasted in data centers is becoming an increasingly important issue\cite{eere}.

The facility costs of large scale HPC systems including cooling and energy usage is becoming more of an issue. Today, 4\% of the energy produced in the world is used in data centers, up from 2\% of energy usage used in data centers 2 years ago\cite{dw,vmware}. 

A solution to addressing the overprovisioning and computational efficiency limitations, as well as hardware and operating costs, of integrated, siloed, systems is the use of Composable Disaggregated Infrastructures (Figure \ref{fig:stranded}).

\begin{figure}
\centerline{\includegraphics[width=\columnwidth]{Slide3.jpeg}}
\caption{More Efficiency in Composable HPC Use of Resources.} 
\label{fig:stranded}
\end{figure}

With Composable Disaggregated Infrastructures, computational resources are not statically provisioned in servers, but instead are physically disaggregated and connected through high-speed/low-latency network fabrics.  These resources can be dynamically provisioned and re-provisioned to client applications, as needed and are thus not only more efficient to manage by removing unnecessary hardware, but help reduce energy consumption and datacenter cooling costs.In this type of architecture, resources are grouped in shared 'pools' that are accessed across high-speed, low latency fabrics (Figure \ref{fig:Pools}). 


\begin{figure}
\centerline{\includegraphics[width=\columnwidth]{pools2.jpeg}}
\caption{Localized Disaggregated Resource Pools Connected by Fabrics.} 
\label{fig:Pools}
\end{figure}
  
Network disaggregation is already common for storage devices (e.g., NVMe-oF); current trends are pushing this paradigm further, extending it to computational engines, memory elements, accelerators and eventually to all forms of compute resources required by modern HPC applications. However, disaggregated resource types are increasingly being accessed over a variety of fabric types and technologies; and being able to fully orchestrate these resources in a dynamic, heterogenous environment requires managing those fabrics and the hardware resources that may be accessed thereon. The management and optimization of such a diverse set of fabrics and fabric technologies to realize the benefits of Composable Disaggregated Infrastructures is quickly becoming a complex issue to solve for infrastructure managers, especially in heterogenous multi-vendor environments, with multiple vendor-sourced hardware and the ever-expanding collection of proprietary APIs and tools. 

Currently, there is no common open-source manager interface or model available to configure the resource pools and the fabrics that link them with applications. So, every tool and every middleware library provider needs unique calls to specific fabric managements stack for each available fabric. An HPC cluster can end up with very diverse administration domains with administrators having to manage each fabric differently through different tools.

The industry needs interoperatbility through common interfaces to enable Composability Managers to efficiently connect workloads with resources in a dynamic ecosystem while being able to the abstractly control the underlying network interconnect. This paper describes the \textit{OpenFabric Management Framework} (OFMF). The OFMF is an open-source API, tool set, and central repository being designed and developed for centralized management of composable resources over dissimilar fabrics, and for manipulation of connected resources using client-friendly abstractions. The OFMF provides a framework that makes it possible to dynamically configure fabric interconects to pair client workloads.

  


