\section{Related Work}
Current HPC architectures have drawbacks due to the fact that they are designed and fabricated out of fixed quantities of discrete compute nodes.  In turn, the compute nodes are designed and assembled with predetermined quantities of CPU cores,  


In parallel, workloads, are becoming highly heterogenous and require increasing amounts of resources (memory, accelerators, etc.) that are not easily found in commodity servers and often require the creation of ad-hoc configurations, further impacting on the sustainability of systems. <https://research.ibm.com/projects/composable-disaggregated-infrastructure#overview>

his unlocks new opportunities to strategically handle resources allocation and eliminate electricity consumption generated by idle hardware and cooling

https://www.itopstimes.com/itops/liqid-introduces-multi-fabric-support-for-composable-infrastructure/

multi-fabric support not open-source

https://venturebeat.com/data-infrastructure/liqid-touts-composable-infrastructure-at-dell-technologies-world/

https://www.reportlinker.com/p05620908/Composable-Infrastructure-Market-by-Type-Vertical-And-Region-Global-Forecast-to.html

The composable infrastructure market size is estimated to be USD 616 million in 2018 and is expected to reach USD 5,102 million by 2023, at a Compound Annual Growth Rate (CAGR) of 52.6% during the forecast period

https://en.wikipedia.org/wiki/Composable_disaggregated_infrastructure

https://www.networkworld.com/article/3266106/what-is-composable-infrastructure.html

https://en.wikipedia.org/wiki/Fungible_Inc.

https://gigaio.com/products/





his unlocks new opportunities to strategically handle resources allocation and eliminate electricity consumption generated by idle hardware and cooling
One early solution was to create technologies specifically to increase the performance of checkpointing, even at the expense of later restarts. Solutions in this space included PLFS~\cite{plfs}, Zest~\cite{zest}, and buddy-checkpointing via SCR~\cite{scr}. These techniques were often regarded as stop-gap solutions, as they decreased checkpoint reliance (or checkpoint requirements) on parallel file systems. However, these techniques did not address the core difficulty of parallel file systems struggling to meet new performance requirements.

The next step was development of new types of storage systems to allow storage workloads to largely avoid parallel file system use. A new class of technology termed ``burst buffers'' was developed~\cite{burstbuffers}. Burst buffers are typically flash-based storage appliances distributed throughout an HPC system, designed to service workloads in a more local fashion. Space within them are often allocated via the job scheduler, allowing some level of performance isolation from other jobs using other burst buffer components in remote parts of the system. Although sometimes still called burst buffers, this term has fallen out of favor because they are now more widely used than the original application, absorbing bursts of I/O generated by checkpoint restart. Instead, these systems (like DataWarp and DDN Infinite Memory Engine) are also used extensively for data analytics, helping resolve the original shared parallel file system contention problem.

A relatively new development is the on-demand parallel file system. BeeGFS-On-Demand~\cite{beeond} is a commonly available instance of this technology. Lustre On Demand~\cite{lustre-on-demand} is a forthcoming design based on this concept. The main idea is to assemble node-local resources (like on-node SSDs and NVMe) to create a parallel file system independent of the centralized file system. This is accomplished by launching file system daemons within each node to serve requests from the clients. One common deployment allows for individual HPC jobs to assemble their own parallel file systems, eliminating parallel file system contention entirely.

While managing parallel file system contention has been a widely discussed topic~\cite{managing-contention}, the impact of I/O processes on compute-bound tasks has not yet been deeply explored. Microkernel research from past decades has highlighted the impact of daemons commonly found in Linux on very large scale, tightly coupled jobs~\cite{daemon-interference}. 
