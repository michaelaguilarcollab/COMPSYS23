\section{Related Work}
As HPC systems have grown, their associated storage systems have been widely identified as a potential bottleneck. One major concern was the viability of the checkpoint-restart pattern of resilience, as compute and memory progress was outpacing parallel file system performance. However, the increasing prominence of data analytics workloads on HPC system presented further risk to the viability of upcoming system architectures, yielding an increased emphasis on the search for alternative solutions.

One early solution was to create technologies specifically to increase the performance of checkpointing, even at the expense of later restarts. Solutions in this space included PLFS~\cite{plfs}, Zest~\cite{zest}, and buddy-checkpointing via SCR~\cite{scr}. These techniques were often regarded as stop-gap solutions, as they decreased checkpoint reliance (or checkpoint requirements) on parallel file systems. However, these techniques did not address the core difficulty of parallel file systems struggling to meet new performance requirements.

The next step was development of new types of storage systems to allow storage workloads to largely avoid parallel file system use. A new class of technology termed ``burst buffers'' was developed~\cite{burstbuffers}. Burst buffers are typically flash-based storage appliances distributed throughout an HPC system, designed to service workloads in a more local fashion. Space within them are often allocated via the job scheduler, allowing some level of performance isolation from other jobs using other burst buffer components in remote parts of the system. Although sometimes still called burst buffers, this term has fallen out of favor because they are now more widely used than the original application, absorbing bursts of I/O generated by checkpoint restart. Instead, these systems (like DataWarp and DDN Infinite Memory Engine) are also used extensively for data analytics, helping resolve the original shared parallel file system contention problem.

A relatively new development is the on-demand parallel file system. BeeGFS-On-Demand~\cite{beeond} is a commonly available instance of this technology. Lustre On Demand~\cite{lustre-on-demand} is a forthcoming design based on this concept. The main idea is to assemble node-local resources (like on-node SSDs and NVMe) to create a parallel file system independent of the centralized file system. This is accomplished by launching file system daemons within each node to serve requests from the clients. One common deployment allows for individual HPC jobs to assemble their own parallel file systems, eliminating parallel file system contention entirely.

While managing parallel file system contention has been a widely discussed topic~\cite{managing-contention}, the impact of I/O processes on compute-bound tasks has not yet been deeply explored. Microkernel research from past decades has highlighted the impact of daemons commonly found in Linux on very large scale, tightly coupled jobs~\cite{daemon-interference}. 
