\section{Related Work}
Current HPC architectures are designed to parallelize fixed quantities of homogenous commodity servers that are implemented as compute hardware.<cite Beowulf>  Each compute server is a fixed quantity of preordained resources, whether it be CPUs, memory, or accelerators.  Heterogenity in compute node features is important to create environments that are adapted to be most energy efficient and computationally performant for individual applications.  However, most of today's HPC Workload Managers are configured to allocate compute node hardware based upon simple differences between these fixed resources compute nodes.  New Workload Managers, such as Fuzzball from Ctrl IQ <cite https://ciq.co/products/fuzzball/hpc/> and Flux <cite https://ipo.llnl.gov/sites/default/files/2022-02/Flux_RD100_Final.pdf#:~:text=Flux%20is%20a%20next-generation%20workload%20management%20framework%20for,%28HPC%29%20clusters%2C%20servers%20in%20the%20cloud%20and%20laptops. > can specify dynamic resources to fulfill the needs of their scheduled Batch jobs.  Because Workload needs can be chosen by the running application, there is an opportunity to dynamically assemble resources to most efficiently complete the computations.
 
As stated earlier in this paper, HPC systems that are architecturally designed out of composable disaggregated resoures, have the potential to save energy, reduce cooling costs, and mitigate run-time performance issues.  Because of the enormous potential, it has been stated that Composable Infrastructure will grow in market size to over 5 billion dollars by 2023. <cite https://www.reportlinker.com/p05620908/Composable-Infrastructure-Market-by-Type-Vertical-And-Region-Global-Forecast-to.html >. <cite https://venturebeat.com/data-infrastructure/liqid-touts-composable-infrastructure-at-dell-technologies-world/ >





https://www.itopstimes.com/itops/liqid-introduces-multi-fabric-support-for-composable-infrastructure/

multi-fabric support not open-source





https://en.wikipedia.org/wiki/Composable_disaggregated_infrastructure

https://www.networkworld.com/article/3266106/what-is-composable-infrastructure.html

https://en.wikipedia.org/wiki/Fungible_Inc.

https://gigaio.com/products/





his unlocks new opportunities to strategically handle resources allocation and eliminate electricity consumption generated by idle hardware and cooling
One early solution was to create technologies specifically to increase the performance of checkpointing, even at the expense of later restarts. Solutions in this space included PLFS~\cite{plfs}, Zest~\cite{zest}, and buddy-checkpointing via SCR~\cite{scr}. These techniques were often regarded as stop-gap solutions, as they decreased checkpoint reliance (or checkpoint requirements) on parallel file systems. However, these techniques did not address the core difficulty of parallel file systems struggling to meet new performance requirements.

The next step was development of new types of storage systems to allow storage workloads to largely avoid parallel file system use. A new class of technology termed ``burst buffers'' was developed~\cite{burstbuffers}. Burst buffers are typically flash-based storage appliances distributed throughout an HPC system, designed to service workloads in a more local fashion. Space within them are often allocated via the job scheduler, allowing some level of performance isolation from other jobs using other burst buffer components in remote parts of the system. Although sometimes still called burst buffers, this term has fallen out of favor because they are now more widely used than the original application, absorbing bursts of I/O generated by checkpoint restart. Instead, these systems (like DataWarp and DDN Infinite Memory Engine) are also used extensively for data analytics, helping resolve the original shared parallel file system contention problem.

A relatively new development is the on-demand parallel file system. BeeGFS-On-Demand~\cite{beeond} is a commonly available instance of this technology. Lustre On Demand~\cite{lustre-on-demand} is a forthcoming design based on this concept. The main idea is to assemble node-local resources (like on-node SSDs and NVMe) to create a parallel file system independent of the centralized file system. This is accomplished by launching file system daemons within each node to serve requests from the clients. One common deployment allows for individual HPC jobs to assemble their own parallel file systems, eliminating parallel file system contention entirely.

While managing parallel file system contention has been a widely discussed topic~\cite{managing-contention}, the impact of I/O processes on compute-bound tasks has not yet been deeply explored. Microkernel research from past decades has highlighted the impact of daemons commonly found in Linux on very large scale, tightly coupled jobs~\cite{daemon-interference}. 
